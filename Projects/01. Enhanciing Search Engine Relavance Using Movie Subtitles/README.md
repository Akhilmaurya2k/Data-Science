# Cine Shazam üé¨üé§

## Overview

Cine Shazam is an interactive Streamlit web application designed to identify movies or TV shows based on recorded audio snippets of their dialogues. If you've ever had a scene stuck in your head and couldn't place the movie, Cine Shazam is here to help! It leverages a Retrieval Augmented Generation (RAG) system built with LangChain and OpenAI to match spoken scenes to its extensive database of subtitles.

## üåü Key Features

* üéôÔ∏è **In-Browser Audio Recording**: Users can easily record audio snippets directly through the web interface.
* üó£Ô∏è **Accurate Speech-to-Text**: Utilizes OpenAI's Whisper model for high-quality transcription of the recorded dialogues.
* üß† **Intelligent Movie/Show Identification**: Employs a sophisticated RAG system. This system uses LangChain, Sentence Transformers for embeddings, and a ChromaDB vector store to find the most relevant movie or TV show based on the transcribed dialogue.
* üé¨ **Detailed Information Output**: Provides the title of the movie/show, its year of release, and, if applicable (for TV series), the season and episode number.

## ‚öôÔ∏è How It Works

The application follows a straightforward pipeline:

1.  **Record**: The user records a short audio clip of a dialogue using the microphone button in the Streamlit app.
2.  **Transcribe**: The recorded audio is then sent to OpenAI's Whisper API, which transcribes the speech into text.
3.  **Query RAG System**: This transcribed text becomes the query for our RAG system.
4.  **Retrieve Context**: The RAG system searches a pre-built ChromaDB vector store (containing embeddings of a vast subtitle dataset) to find the most similar subtitle chunks corresponding to the query.
5.  **Generate & Display**: The retrieved subtitle chunks (context) and the original query are passed to an OpenAI LLM (e.g., GPT-3.5-turbo). The LLM, guided by a specific prompt template, identifies the movie/show and extracts relevant details (title, year, season/episode). This information is then displayed to the user.

## üõ†Ô∏è Technologies Used

* **Python**
* **Streamlit**: For the interactive web application interface.
* **LangChain**: Framework for developing applications powered by language models, used here to build the RAG chain.
* **OpenAI**:
    * **Whisper**: For speech-to-text transcription.
    * **Chat Models (GPT-3.5-turbo / GPT-4o-mini)**: For generating the final answer based on retrieved context.
* **Sentence Transformers (`all-MiniLM-L6-v2`)**: For generating dense vector embeddings of subtitle text.
* **ChromaDB**: Vector database used to store and efficiently search subtitle embeddings.
* **Pandas**: For data manipulation during the data preparation phase.
* **SQLite**: The initial subtitle data is stored in an SQLite database.
* **NLTK**: Used for text cleaning (tokenization, lemmatization) in the data preparation notebook.
* **Pydub, Sounddevice**: For handling audio recording in the Streamlit application.
* **Jupyter Notebooks**: For the data preparation pipeline and the development/testing of the RAG system.

## üìö Core Components

* **`app.py`**: The main Python script for the Streamlit web application. Contains the UI logic, audio recording, transcription, and RAG chain invocation.
* **`1. Data Preperation.ipynb`**: A Jupyter Notebook that details the process of:
    * Loading raw subtitle data from an SQLite database (`eng_subtitles_database.db`).
    * Decoding, cleaning (regex, lowercasing, lemmatization), and processing subtitle text.
    * Extracting metadata such as movie/series name, season, episode, and year from filenames.
    * Saving the processed data into a CSV file (`data/subtitles_metadata.csv`).
* **`2. Building Movie Shazam with LangChain.ipynb`**: A Jupyter Notebook that outlines the construction and testing of the RAG system:
    * Loading the processed subtitle data (expects `data/subtitles.csv`).
    * Further metadata processing and document structuring.
    * Splitting documents into manageable chunks for embedding.
    * Generating embeddings using Sentence Transformers and storing them in a ChromaDB collection (`vector_database`) in batches.
    * Defining and testing the LangChain RAG pipeline with an OpenAI model (e.g., `gpt-4o-mini`).
* **`data/`**: This directory is intended to hold:
    * `eng_subtitles_database.db`: The initial raw database of zipped subtitles (not provided in this repo, user must source this).
    * `subtitles_metadata.csv`: Output of `1. Data Preperation.ipynb`.
    * `subtitles.csv`: The input file for `2. Building Movie Shazam with LangChain.ipynb`. You might need to rename or ensure this is correctly derived from `subtitles_metadata.csv`.
* **`chroma_db_/`**: Directory where ChromaDB persists the vector store and collection data. This is generated by `2. Building Movie Shazam with LangChain.ipynb`.
* **`keys/`**: Directory to store API keys.
    * `openai_key.txt`: A plain text file containing your OpenAI API key.

## üöÄ Setup and Installation

1.  **Clone the Repository**:
    ```bash
    git clone <Projects/01. Enhanciing Search Engine Relavance Using Movie Subtitles>
    ```

2.  **Create a Virtual Environment and Activate It**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install Dependencies**:
    Create a `requirements.txt` file with all necessary packages (based on the imports in `app.py` and the Jupyter Notebooks). Example packages include:
    ```
    streamlit
    sounddevice
    pydub
    openai
    langchain
    langchain-openai
    langchain-chroma
    sentence-transformers
    torch
    pandas
    nltk
    tqdm
    # Add any other specific versions if necessary
    ```
    Then install them:
    ```bash
    pip install -r requirements.txt
    ```
    You may also need to download NLTK resources:
    ```python
    import nltk
    nltk.download('punkt')
    nltk.download('wordnet')
    ```

4.  **Obtain OpenAI API Key**:
    * Create a directory named `keys`.
    * Inside `keys`, create a file named `openai_key.txt`.
    * Paste your OpenAI API key into this file.

5.  **Data Preparation (`1. Data Preperation.ipynb`)**:
    * Place your English subtitles database file (e.g., `eng_subtitles_database.db` containing a `zipfiles` table with 'num', 'name', 'content' columns) into the `data/` directory.
    * Run the `1. Data Preperation.ipynb` notebook. This will process the raw subtitles and generate `data/subtitles_metadata.csv`.
    * **Note**: The second notebook (`2. Building Movie Shazam with LangChain.ipynb`) expects `data/subtitles.csv`. Ensure this file is correctly named/created based on the output of the first notebook (e.g., by renaming `subtitles_metadata.csv` or adjusting the path in the second notebook).

6.  **Build Vector Database (`2. Building Movie Shazam with LangChain.ipynb`)**:
    * Run the `2. Building Movie Shazam with LangChain.ipynb` notebook.
    * This notebook will:
        * Load the data from `data/subtitles.csv` (it samples 30% by default, you can adjust this).
        * Process and chunk the documents.
        * Generate embeddings using `all-MiniLM-L6-v2`.
        * Create and populate a ChromaDB vector store in the `./chroma_db_/` directory.

## ‚ñ∂Ô∏è Usage (Running the Streamlit App)

Once all the setup steps, especially data preparation and vector database creation, are completed:

1.  Ensure your virtual environment is activated.
2.  Run the Streamlit application from your terminal:
    ```bash
    streamlit run app.py
    ```
3.  Open your web browser and navigate to the local URL provided by Streamlit (usually `http://localhost:8501`).
4.  Click the "üéôÔ∏è Record" button to start recording audio. Speak the dialogue you want to identify. Click the button again to stop recording.
5.  The application will transcribe the audio. Then, it will search its database and display the identified movie or TV show title, year, and season/episode if applicable.

## üìù Prompt Engineering

The RAG chain in `app.py` uses a specific prompt template to guide the LLM's output. This template instructs the model to provide:
1.  The title of the movie, web series, or TV show.
2.  The year of release (only if found in context).
3.  The season and episode number (only for web series and if found).

It strictly tells the model to avoid extra information, justifications, or placeholder phrases like "Not specified," "N/A," etc., ensuring a clean output.

## üí° Future Enhancements

* Allow uploading audio files instead of only live recording.
* Expand the subtitle dataset for wider coverage.
* Implement more advanced RAG techniques for better accuracy (e.g., re-ranking, query expansion).
* Add links to movie databases (IMDb, TMDB) for the identified content.
* User accounts and history of identified movies.

## ü§ù Contributors

* Akhil Padma 
